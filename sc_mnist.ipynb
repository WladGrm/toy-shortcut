{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioAv6TYPC7kL",
        "outputId": "b40ef08e-ff54-48be-bc11-004b5722d10f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install torchdyn\n",
        "!pip -q install torchcfm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiwGSq3BA1gr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm import trange\n",
        "import wandb\n",
        "\n",
        "from torchdyn.core import NeuralODE\n",
        "from torchcfm.models.unet.unet import UNetModelWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WnSMs0aELkS"
      },
      "outputs": [],
      "source": [
        "def warmup_lr(step, WARMUP=5000):\n",
        "    \"\"\"Simple warmup schedule from 0 to initial LR over first WARMUP steps.\"\"\"\n",
        "    return min(step, WARMUP) / WARMUP\n",
        "\n",
        "def infiniteloop(dataloader):\n",
        "    \"\"\"Creates an infinite iterator over a given dataloader.\"\"\"\n",
        "    while True:\n",
        "        for x, _ in dataloader:\n",
        "            yield x\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_samples_euler(model,\n",
        "                           savedir=\"./results_mnist/\",\n",
        "                           step_=0,\n",
        "                           total_steps=100,\n",
        "                           net_=\"normal\",\n",
        "                           plot=False):\n",
        "    \"\"\"\n",
        "    Generate and save 32 samples using a simple Euler iteration from t=0 to t=1.\n",
        "    Article reference:\n",
        "      x ∼ N(0, I)\n",
        "      d ← 1/M\n",
        "      t ← 0\n",
        "      for n in [0..M−1]:\n",
        "         x ← x + sθ(x, t, d)*d\n",
        "         t ← t + d\n",
        "      return x\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1) Sample random Gaussian as our initial x\n",
        "    B = 32\n",
        "    x = torch.randn(B, 1, 28, 28, device=device)\n",
        "\n",
        "    # 2) Set up step size d = 1 / total_steps\n",
        "    dt = 1.0 / total_steps\n",
        "    t = 0.0\n",
        "\n",
        "    # 3) Simple Euler loop\n",
        "    for _ in range(total_steps):\n",
        "        # Compute the velocity/score sθ(x, t, d).\n",
        "        s = model(t, x, d=dt)\n",
        "\n",
        "        # Update x ← x + sθ(x, t, d)*d\n",
        "        x = x + s * dt\n",
        "\n",
        "        # Update t ← t + d\n",
        "        t += dt\n",
        "\n",
        "    # 4) Post-process: clamp & shift from [-1..1] to [0..1]\n",
        "    x_gen = x.clamp(-1, 1)\n",
        "    x_gen = x_gen / 2 + 0.5\n",
        "\n",
        "    # 5) Save out\n",
        "    os.makedirs(savedir, exist_ok=True)\n",
        "    img_path = f\"{savedir}/{net_}_generated_EULER_step_{step_}_ts_{total_steps}.png\"\n",
        "    save_image(x_gen, img_path, nrow=8)\n",
        "\n",
        "    if plot:\n",
        "        import matplotlib.pyplot as plt\n",
        "        from torchvision.utils import make_grid\n",
        "        grid = make_grid(x_gen, nrow=8)\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(grid.permute(1,2,0).cpu().numpy(), cmap=\"gray\")\n",
        "        plt.title(f\"MNIST Euler Gen: {net_} | step={step_}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "def ema(source, target, decay=0.9999):\n",
        "    \"\"\"EMA update of target's params from source.\"\"\"\n",
        "    source_dict = source.state_dict()\n",
        "    target_dict = target.state_dict()\n",
        "    for k in source_dict.keys():\n",
        "        target_dict[k].data.copy_(\n",
        "            target_dict[k].data * decay + source_dict[k].data * (1 - decay)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J38wmqzuENzK"
      },
      "outputs": [],
      "source": [
        "############################\n",
        "# 2) MNIST Data Loader\n",
        "############################\n",
        "def get_mnist_dataloader(batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        # NO RESIZE -> keep 28×28\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Single-channel normalization\n",
        "    ])\n",
        "\n",
        "    mnist_train = datasets.MNIST(\n",
        "        root=\"./data_mnist\",\n",
        "        train=True,\n",
        "        transform=transform,\n",
        "        download=True\n",
        "    )\n",
        "    loader = DataLoader(\n",
        "        mnist_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        drop_last=True,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlJ6ClOQERYp",
        "outputId": "c7d386b3-ea06-4afa-bd32-578c87ee10d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model params: 15.19 M\n"
          ]
        }
      ],
      "source": [
        "###########################\n",
        "# 3) Define U-Net for 1×28×28\n",
        "############################\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torch.nn as nn\n",
        "from torchcfm.models.unet.unet import UNetModelWrapper\n",
        "from torchcfm.models.unet.nn import timestep_embedding\n",
        "\n",
        "\n",
        "class UNetModelWrapperWithD(UNetModelWrapper):\n",
        "    \"\"\"\n",
        "    A subclass of `UNetModelWrapper` that can optionally take an extra param 'd' and\n",
        "    incorporate it into the time embedding. If d=None, it behaves exactly like the original model.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, t, x, d=None, y=None, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        :param t: timesteps, shape [B], or [B, 1].\n",
        "        :param x: the input image tensor, shape [B, C, H, W].\n",
        "        :param d: optional extra scalar(s), shape [B], [B,1], etc. If None, we skip it.\n",
        "        :param y: optional class labels (if model is class-conditional).\n",
        "        :return: an output image of shape [B, C, H, W].\n",
        "        \"\"\"\n",
        "\n",
        "        #1) Flatten out the timesteps if needed \n",
        "        timesteps = t\n",
        "        while timesteps.dim() > 1:\n",
        "            timesteps = timesteps[:, 0]\n",
        "        if timesteps.dim() == 0:\n",
        "            timesteps = timesteps.repeat(x.shape[0])\n",
        "\n",
        "        # 2) Timestep embedding\n",
        "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
        "\n",
        "        # 3) If class-conditional, add label embedding \n",
        "        if self.num_classes is not None:\n",
        "            # parent class requires y if the model is class-cond\n",
        "            assert (y is not None), \"You must pass 'y' if the model is class-conditional\"\n",
        "            emb = emb + self.label_emb(y)\n",
        "\n",
        "        #4) If d is provided, embed it and add to emb\n",
        "        if d is not None:\n",
        "            d_ = d\n",
        "            while d_.dim() > 1:\n",
        "                d_ = d_[:, 0]\n",
        "            if d_.dim() == 0:\n",
        "                d_ = d_.repeat(x.shape[0])\n",
        "\n",
        "            d_emb = self.time_embed(timestep_embedding(d_, self.model_channels))\n",
        "            emb = emb + d_emb \n",
        "\n",
        "\n",
        "        h = x.type(self.dtype)\n",
        "        hs = []\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb)\n",
        "            hs.append(h)\n",
        "\n",
        "        h = self.middle_block(h, emb)\n",
        "\n",
        "        for module in self.output_blocks:\n",
        "            h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = module(h, emb)\n",
        "\n",
        "        h = h.type(x.dtype)\n",
        "        out = self.out(h)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "net_model = UNetModelWrapperWithD(\n",
        "    dim=(1, 28, 28),      # 1 channel, 28×28\n",
        "    num_res_blocks=2,\n",
        "    num_channels=128,      \n",
        "    channel_mult=[1, 2],  # just one downsampling: 28->14\n",
        "    num_heads=4,\n",
        "    num_head_channels=64,\n",
        "    attention_resolutions=\"14\",\n",
        "    dropout=0.05,\n",
        ").to(device)\n",
        "\n",
        "\n",
        "ema_model = copy.deepcopy(net_model)\n",
        "\n",
        "# Print model size\n",
        "model_size = sum(p.numel() for p in ema_model.parameters())\n",
        "print(\"Model params: %.2f M\" % (model_size / 1024 / 1024))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrU1mBAAES9K"
      },
      "outputs": [],
      "source": [
        "############################\n",
        "# 4) Flow Matching Trainer\n",
        "############################\n",
        "import torch.nn as nn\n",
        "\n",
        "def pick_discrete_steps(batch_size, device):\n",
        "    \"\"\"\n",
        "    Returns a tensor of shape [batch_size] containing\n",
        "    discrete step sizes from the set {1/2, 1/4, 1/8, 1/16, 1/32, 1/64, 1/128}.\n",
        "    Each is sampled with equal probability.\n",
        "    \"\"\"\n",
        "    # 1) define the set: [2,4,8,16,32,64,128]\n",
        "    #    then invert => [1/2,1/4,...,1/128].\n",
        "    powers_of_two = torch.tensor([2,4,8,16,32,64,128], device=device, dtype=torch.float)\n",
        "    possible_steps = 1.0 / powers_of_two  # shape [7]\n",
        "\n",
        "    # 2) pick random indices in [0..6] for each batch element\n",
        "    #    uniform discrete distribution\n",
        "    random_inds = torch.randint(0, len(possible_steps), (batch_size,), device=device)\n",
        "    # 3) gather the step sizes\n",
        "    selected_steps = possible_steps[random_inds]  # shape [batch_size]\n",
        "\n",
        "    return selected_steps\n",
        "\n",
        "\n",
        "def train_flowmatching_shortcut(\n",
        "    net_model,\n",
        "    ema_model,\n",
        "    TOTAL_STEPS=50000,\n",
        "    BATCH_SIZE=64,\n",
        "    LR=2e-4,\n",
        "    GRAD_CLIP=1.0,\n",
        "    SAVE_STEP=1000,\n",
        "    WARMUP=5000,\n",
        "    # fraction of each batch used for standard flow matching (d=0)\n",
        "    FRACTION_K=0.75\n",
        "):\n",
        "    loader = get_mnist_dataloader(batch_size=BATCH_SIZE)\n",
        "    data_iter = infiniteloop(loader)\n",
        "\n",
        "    # Basic setup: optimizer & scheduler\n",
        "    optimizer = torch.optim.AdamW(net_model.parameters(), lr=LR, weight_decay=0.1)\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "        optimizer, lr_lambda=lambda s: warmup_lr(s, WARMUP=WARMUP)\n",
        "    )\n",
        "\n",
        "    # MSE for velocity fields\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    # Close previous W&B run if present\n",
        "    if wandb.run is not None:\n",
        "        wandb.finish()\n",
        "    wandb.init(project=\"FM-Shortcut\", config={\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"lr\": LR,\n",
        "        \"frac_k\": FRACTION_K,\n",
        "        \"total_steps\": TOTAL_STEPS,\n",
        "    })\n",
        "\n",
        "    # Main training loop\n",
        "    pbar = trange(TOTAL_STEPS, desc=\"ShortcutTrainer\", dynamic_ncols=True)\n",
        "    for step in pbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ====== 1) Load data batch ======\n",
        "        x_real = next(data_iter).to(device)   # shape: [B,1,28,28] for MNIST\n",
        "        B_ = x_real.size(0)\n",
        "\n",
        "        # ====== 2) Sample x_0 ~ N(0,I), sample random t in [0..1] ======\n",
        "        x_noise = torch.randn_like(x_real)\n",
        "        t_sample = torch.rand(B_, device=device)\n",
        "        t_4d = t_sample.view(B_,1,1,1)\n",
        "        # interpolation\n",
        "        x_t = (1 - t_4d)*x_noise + t_4d*x_real\n",
        "\n",
        "        # ====== 3) Pick discrete step sizes {1/2,1/4,1/8,...} + set fraction K to zero ======\n",
        "        d_vals = pick_discrete_steps(B_, device=device)\n",
        "        K = int(B_ * FRACTION_K)\n",
        "        if K>0:\n",
        "            d_vals[:K] = 0.0  # the first K items do standard FM\n",
        "\n",
        "        # ====== 4) Flow Matching Loss for the first K items (d=0) ======\n",
        "        fm_loss = 0.0\n",
        "        if K>0:\n",
        "            pred_vel_fm = net_model(t_sample[:K], x_t[:K], d=d_vals[:K])\n",
        "            target_vel_fm = (x_real[:K] - x_noise[:K])  # x_1 - x_0\n",
        "            fm_loss = loss_fn(pred_vel_fm, target_vel_fm)\n",
        "\n",
        "        # ====== 5) Self-Consistency for items K..B-1 (where d>0) ======\n",
        "        consistency_loss = 0.0\n",
        "        if K < B_:\n",
        "            idx = torch.arange(K, B_, device=device)\n",
        "            t_cons = t_sample[idx]\n",
        "            x_t_cons = x_t[idx]\n",
        "            d_cons = d_vals[idx]\n",
        "\n",
        "            # Step 1: s_t\n",
        "            s_t = net_model(t_cons, x_t_cons, d=d_cons)\n",
        "\n",
        "            # x_{t+d} = x_t + s_t*d\n",
        "            x_next = x_t_cons + s_t * d_cons.view(-1,1,1,1)\n",
        "            # clamp time at <=1\n",
        "            t_next = torch.clamp(t_cons + d_cons, max=1.0)\n",
        "\n",
        "            # Step 2: s_{t+d}\n",
        "            s_t_next = net_model(t_next, x_next, d=d_cons)\n",
        "\n",
        "            # Step 3: average => s_target\n",
        "            s_target = 0.5*(s_t + s_t_next)\n",
        "\n",
        "            # Step 4: predict net_model(t, x_t, 2*d)\n",
        "            pred_s = net_model(t_cons, x_t_cons, d=2.0*d_cons)\n",
        "\n",
        "            consistency_loss = loss_fn(pred_s, s_target)\n",
        "\n",
        "        total_loss = fm_loss + consistency_loss\n",
        "        total_loss.backward()\n",
        "\n",
        "        # grad clip\n",
        "        nn.utils.clip_grad_norm_(net_model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # EMA update\n",
        "        ema(net_model, ema_model)\n",
        "\n",
        "        # Logging\n",
        "        wandb.log({\n",
        "            \"step\": step,\n",
        "            \"fm_loss\": float(fm_loss),\n",
        "            \"consistency_loss\": float(consistency_loss),\n",
        "            \"total_loss\": float(total_loss),\n",
        "        })\n",
        "        pbar.set_postfix({\n",
        "            \"fm\": f\"{fm_loss:.4f}\",\n",
        "            \"cons\": f\"{consistency_loss:.4f}\",\n",
        "            \"loss\": f\"{total_loss:.4f}\"\n",
        "        })\n",
        "\n",
        "        # Periodic save & sample\n",
        "        if SAVE_STEP > 0 and (step % SAVE_STEP == 0) and (step>0):\n",
        "            os.makedirs(\"./checkpoints_mnist/\", exist_ok=True)\n",
        "\n",
        "            generate_samples(\n",
        "                net_model,\n",
        "                step_=step,\n",
        "                savedir=\"./checkpoints_mnist/img/normal/\",\n",
        "                total_steps=4,\n",
        "                net_=\"normal\"\n",
        "            )\n",
        "            generate_samples(\n",
        "                ema_model,\n",
        "                step_=step,\n",
        "                savedir=\"./checkpoints_mnist/img/ema/\",\n",
        "                total_steps=4,\n",
        "                net_=\"ema\"\n",
        "            )\n",
        "\n",
        "            ckpt_path = f\"./checkpoints_mnist/fm_mnist_step_{step}.pth\"\n",
        "            torch.save({\n",
        "                \"model\": net_model.state_dict(),\n",
        "                \"ema_model\": ema_model.state_dict(),\n",
        "                \"sched\": scheduler.state_dict(),\n",
        "                \"optim\": optimizer.state_dict(),\n",
        "                \"step\": step,\n",
        "            }, ckpt_path)\n",
        "            wandb.save(ckpt_path)\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "Pm7Q9ZgpEUw2",
        "outputId": "c6b4df72-deb7-4a34-cccd-e31f7f53167a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250407_205743-ltojqrz2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut/runs/ltojqrz2' target=\"_blank\">serene-jazz-7</a></strong> to <a href='https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut' target=\"_blank\">https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut/runs/ltojqrz2' target=\"_blank\">https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut/runs/ltojqrz2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ShortcutTrainer: 100%|██████████| 60001/60001 [2:57:46<00:00,  5.63it/s, fm=0.1476, cons=0.0008, loss=0.1484]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>consistency_loss</td><td>▁▃▂▂▂▃▁▂▂▁█▃▂▁▃▂▃▂▂▂▂▂▂▁▁▃▁▂▄▂▂▂▁▃▂▁▂▂▂▂</td></tr><tr><td>fm_loss</td><td>▃▅▁▅▆▇▄▄▆▅▇▅▇▇▅▄▅▆▆▃▆▇▅▇▅▄▅▄▅█▃▆▅▅▆▅▄▃▁█</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>total_loss</td><td>▃▃▇▃▃▆▄▅▅▆▃▃▁█▆▆▂▃▅▃▃▂▅▄▃▅▄▅▅▇▁▂▄▄▆▂▄▆▇▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>consistency_loss</td><td>0.00081</td></tr><tr><td>fm_loss</td><td>0.14763</td></tr><tr><td>step</td><td>60000</td></tr><tr><td>total_loss</td><td>0.14844</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">serene-jazz-7</strong> at: <a href='https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut/runs/ltojqrz2' target=\"_blank\">https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut/runs/ltojqrz2</a><br> View project at: <a href='https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut' target=\"_blank\">https://wandb.ai/wladgrm-skolkovo-institute-of-science-and-technology/fm-shortcut</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 60 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250407_205743-ltojqrz2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "############################\n",
        "# 5) Run Training\n",
        "############################\n",
        "if __name__ == \"__main__\":\n",
        "    train_flowmatching_shortcut(\n",
        "        net_model = net_model,\n",
        "        ema_model = ema_model,\n",
        "        TOTAL_STEPS=60001,\n",
        "        BATCH_SIZE=64,\n",
        "        LR=1e-4,\n",
        "        GRAD_CLIP=1.0,\n",
        "        SAVE_STEP=1000,\n",
        "        WARMUP=3000\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
